---
title: "Un outil d'analyse exploratoire avec Tableau et R"
author: '[Simon Keith](Profil publichttps://fr.linkedin.com/in/simonkth), [Actinvision](http://www.actinvision.fr/)'
date: "16 mars 2016"
output:
  html_document:
    highlight: zenburn
    keep_md: yes
    theme: united
---
<br>  

# Introduction
### Vous avez dit analyse exploratoire ?
...  
<br>  

### Pourquoi Tableau est un bon candidat
...  
<br>  

### Prérequis
Tous des fichiers sont disponible dans ce [répertoire Github](https://github.com/simonkth/TableauR_ExploratoryAnalysis).  
Si vous souhaitez reproduire cet outil, assurez-vous de remplir les prérequis suivants avant de continuer :  

* avoir installé une version de Tableau Desktop supportant l'intégration avec R (8.1 ou supérieure)  
      + vous pouvez vous procurer une version d'essai à [cette adresse](http://get.tableau.com/fr-fr/partner-trial.html?partner=29294)  
* avoir installé R version 3.0.2 ou supérieure (disponible [ici](https://www.r-project.org/))  
* avoir ajouté R à votre variable d'environnement _path_ (par exemple, "_C:\\Program Files\\R\\R-3.0.2\\bin\\x64_")  
* avoir lancé Rserve en tâche d'arrière plan  
      + pour une configuration rapide et suffisante dans notre cas, téléchargez le contenu du [répertoire Github](https://github.com/simonkth/TableauR_ExploratoryAnalysis) et double-cliquez sur _Rserve.cmd_ dans le dossier _Rserve_ (ne fermez pas la fenêtre qui s'ouvre)  
      + pour une installation plus durable et plus flexible, suivez [ces instructions](http://kb.tableau.com/articles/knowledgebase/r-implementation-notes?lang=fr-fr)  
<br>  

# ...
...  
<br>  

# Jeu de données
### Origine et description
Le jeu de données que nous allons utiliser pour cette démonstration provient de la plateforme ouverte des [données publiques françaises](https://www.data.gouv.fr/fr/). Il contient un certain nombre d'indicateurs concernant l'insertion professionnelle des diplômés de Master en universités, fournis par le site de l'[enseignement supérieur français](www.enseignementsup-recherche.gouv.fr).  

Le jeu de données brut et sa documentation sont disponible [ici](https://www.data.gouv.fr/fr/datasets/insertion-professionnelle-des-diplomes-de-master-en-universites-et-etablissements-assimil-0/).  
<br>  

### Nettoyage et préparation avec R
Si le travail de préparation des données ne vous intéresse pas et que vous souhaitez directement attaquer les manipulations dans Tableau, vous pouvez passer cette section.  

Sinon, n'oubliez pas de configurer au préalable votre répertoire de travail avec la fonction `setwd`.  

Nous commençons par télécharger les données, puis nous réglons les problèmes de qualité des données. Nous supprimons ensuite les cas où le nombre de réponses est inférieur à 30, ainsi que ceux où le taux de réponse est inférieur à 30%. Nous sélectionnons également les colonnes qui nous intéressent. Enfin, nous exportons un fichier _.csv_ pour Tableau et nous affichons un échantillon des données.  
```{r dl_clean, cache=TRUE, message=FALSE}
# load packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(downloader, data.table, dplyr)

# download and load the data (Windows setup)
if(!file.exists("./data/insertion_raw.csv")) {
      if(!file.exists("./data")) {dir.create("./data")}
      dataUrl <- paste0("https://data.enseignementsup-recherche.gouv.fr/explore/dataset/", 
                        "fr-esr-insertion_professionnelle-master/download?format=csv")
      download(dataUrl, dest="./data/insertion_raw.csv", mode="wb")
} else message("The dataset had previously been downloaded.")

# read data with fread(), with list of strings to ignore
insertion <- fread("./data/insertion_raw.csv", sep=";", encoding="UTF-8", 
                  na.strings=c("NA", "ns", "nd", "fe", ".", paste0(1, "\U00A0", 710), ""))

# remove data if sample is too small (nombre_de_reponses < 30) 
# or if the response rate is too low (taux_de_reponse < 30)
# also remove fields that we don't want to keep in Tableau
insertion <- insertion %>% 
      filter(nombre_de_reponses >= 30, taux_de_reponse >= 30, 
             numero_de_l_etablissement != "UNIV") %>%
      select(-c(numero_de_l_etablissement, code_de_l_academie, code_du_domaine, 
                code_de_la_discipline, remarque, cle_etab, cle_disc))

# export data in csv for Tableau
# note: we remove R "NAs" and replace with empty cells since Tableau doesn't handle them
insertion_noNa <- insertion
insertion_noNa[is.na(insertion_noNa)] <- ""
write.csv(insertion_noNa, "./data/insertion_Tableau.csv", row.names=FALSE)
remove(insertion_noNa)

# show sample
str(insertion)
```
<br>  

# Développement des outils d'analyse
### ...
...  
<br>  

### Nuage de points et densitées marginales
L'un de mes premiers réflexes lorsque j'explore un nouveau jeu de données est de rechercher des relation entre les différentes variables. Pour les variables quantitatives, le nuage de point est un incontournable. Sur ce type de graphique, j'ai également tendance à utiliser la couleur pour comparer les différents niveaux d'une variable qualitative (ou dimension).  

Le problème avec les nuages de points, c'est qu'ils ont tendance à devenir rapidement illisibles lorsqu'il y a beaucoup d'éléments à afficher. Il est donc intéressant d'enrichir ces vues. On peut ainsi y ajouter différents éléments : une courbe de tendance, le résultat d'un test statistique, ou encore les densités marginales des nos variables...  

Ci-dessous une petite démonstration rapide en R avec le package _ggplot2_. J'ai ajouté une courbe de régression [LOESS](https://en.wikipedia.org/wiki/Local_regression), le résultat d'un [test de corrélation de Pearson](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) ainsi que les [distributions marginales](https://en.wikipedia.org/wiki/Marginal_distribution) déclinées en couleur sur les différents domaines d'études proposés par nos universités. C'est une démonstration rapide, je n'ai donc pas pris la peine d'afficher la légende. Vous remarquerez aussi que cela demande beaucoup de code, et que l'alignement des différents éléments reste assez approximatif.  
```{r scatterMargins, warning=FALSE, fig.height=6, fig.width=9, fig.align='center'}
# load packages for graphics
pacman::p_load(ggplot2, gridExtra)

# set variables
graphData <- data.frame(
      axisX=insertion$femmes,
      axisY=insertion$salaire_net_median_des_emplois_a_temps_plein, 
      colDim=insertion$domaine
      )
graphData <- graphData[complete.cases(graphData),]
axisNames <- c("Pourcentage de femmes", "Salaire net médian")

# helper function for graphic themes
myTheme <- function(...) theme(legend.position="none", 
                               panel.background=element_blank(), 
                               panel.grid.major=element_blank(), 
                               panel.grid.minor=element_blank(), 
                               panel.margin=unit(0, "null"), 
                               axis.ticks=element_blank(), 
                               axis.text.x=element_blank(), 
                               axis.text.y=element_blank(), 
                               axis.title.x=element_blank(), 
                               axis.title.y=element_blank(), 
                               panel.border=element_rect(color=NA), ...)

# plotting Pearson's correlation test
corTest <- with(graphData, cor.test(axisX, axisY, method="pearson"))
corText <- paste0("Pearson's r: ", formatC(corTest$estimate, digits=4, format="f"), 
                  "\np value: ", formatC(corTest$p.value, digits=4, format="e"))
gText <- ggplot() + 
  annotate("text", x=0, y=0, size=4, label=corText) + 
  theme_bw() + myTheme()

# scatterplot with LOESS smooth line
g1 <- ggplot(graphData, aes(x=axisX, y=axisY, colour=factor(colDim))) +
      geom_point(alpha=0.4, size=1.5) + 
      geom_smooth(aes(x=axisX, y=axisY), 
                  inherit.aes=FALSE, method="loess") +
      scale_x_continuous(expand=c(0.01, 0.01)) +
      scale_y_continuous(expand=c(0.01, 0.01)) +
      theme_bw() + xlab(axisNames[1]) + ylab(axisNames[2]) +
      theme(legend.position="none", plot.margin=unit(c(1, 1, 1, 1), "points"))

# x marginal density
g2 <- ggplot(graphData, aes(x=axisX, colour=factor(colDim), fill=factor(colDim))) + 
  geom_density(alpha=0.4) + 
  scale_x_continuous(breaks=NULL, expand=c(0.01, 0.01)) +
  scale_y_continuous(breaks=NULL, expand=c(0, 0)) +
  theme_bw() +
  myTheme(plot.margin=unit(c(0, 0, 0, 3), "lines")) 

# y marginal density
g3 <- ggplot(graphData, aes(x=axisY, colour=factor(colDim), fill=factor(colDim))) + 
  geom_density(alpha=0.4) + 
  coord_flip()  + 
  scale_x_continuous(labels=NULL, breaks=NULL, expand=c(0.01, 0.01)) +
  scale_y_continuous(labels=NULL, breaks=NULL, expand=c(0, 0)) +
  theme_bw() +
  myTheme(plot.margin=unit(c(0, 0, 2.1, 0), "lines"))

# arrange plots
remove(graphData, axisNames)
grid.arrange(arrangeGrob(g2, gText, ncol=2, widths=c(3, 1)), 
             arrangeGrob(g1, g3, ncol=2, widths=c(3, 1)), 
             heights=c(1, 3))
```
<br>  

Bien entendu ce code pourrait être largement amélioré (ajout de la légende, alignement dynamique des différents éléments, etc.) mais vous avez compris le problème : c'est difficile à maintenir, et surtout cela manque cruellement d’interactivité. Je peux changer mes variables, mais pour cela je dois modifier puis relancer mon code. De plus, je ne suis pas sûr que l'alignement de mes graphiques restera bon. Je n'ai pas non plus la possibilité d'ajouter des infobulles ni de filtrer mes données... Bref, et si l'on essayait de porter tout cela dans Tableau ?  
<br>  

#### Mise en place
Vous trouverez le classeur Tableau dans le [répertoire Github](https://github.com/simonkth/TableauR_ExploratoryAnalysis) (_TableauR_ExploratoryAnalysis_FR.twb_). Je ne vais pas détailler tous les champs calculés, aussi je vous conseille de travailler avec deux fenêtres de Tableau. Dans la première, ouvrez mon classeur et jetez y un coup d’œil de temps en temps pour voir comment je m'y suis pris. Dans la deuxième, ouvrez un classeur vierge et recommencez à partir de zéro.  

La première étape est de se connecter aux données (fichier _insertion_Tableau.csv_). N'hésitez pas à renommer les champs et à formater correctement les mesures, comme je l'ai fait. Puis nous passons au premier élément, le nuage de points.  
<br>  

#### Nuage de points amélioré
D'abord, n'oubliez pas de désactiver l'agrégation dans _Analyse_ $\rightarrow$ _Agréger les mesures_. Ensuite, le premier réflexe est de dynamiser notre vue en créant un certain nombre de paramètres, ainsi que les champs paramétrables correspondants. Faites _clique-droit_ $\rightarrow$ _modifier_ pour comprendre comment ils sont construits. Pour le nuage de points, nous avons donc :  

* le paramètre __Abscisse__ qui commande le champ __X__  
* le paramètre __Ordonnée__ qui commande le champ __Y__  
* les paramètres __Niveaux__ et __Niveaux maximum__ qui commandent les champs __Niveaux pour couleur__ et __Couleur__  

Grâce aux deux premiers paramètres, je peux choisir les mesures que je souhaite placer en abscisse et en ordonnée. Quelques précisions s'imposent pour le dernier point. Grâce à __Niveaux__, je peux choisir une dimension pour déterminer la couleur des points (via le champ __Niveaux pour couleur__). Cependant je ne contrôle pas le nombre de niveaux que contient cette dimension ! Par sécurité, je créé un nouveau couple paramètre / champ calculé (__Niveaux maximum__ et __Couleur__) qui va me permettre de n'afficher des couleurs que si le nombre de niveaux ne dépasse pas un certain seuil. C'est donc le champ __Couleur__ que je place sur mon repère des couleurs. En revanche, je peux utiliser le champ __Niveaux pour couleur__ pour filtrer ma dimension. Ainsi, bien qu'il y ait 28 niveaux pour la dimension __Académie__, si je la sélectionne puis que je filtre pour ne garder que 6 niveaux (et que __Niveaux maximum__ est fixé à 6), les académies que j'ai sélectionné vont bel et bien s'afficher en couleur. En revanche si j'ajoute une académie de trop à ma sélection, Tableau repasse sur une couleur unique.  

Je filtre également mes champs __X__ et __Y__ pour exclure les cas où les valeur sont manquantes.  

![Nuage de points dynamique dans Tableau](figures/nuage_de_points.png)  
<br>  

Passons à la courbe de tendance. Tableau propose déjà une solution, cependant l'idéal serait de pouvoir ajouter d'autres possibilités, comme la régression LOESS par exemple. Pour cela on va utiliser un paramètre permettant de choisir le type de modèle, et un script R pour réaliser les calculs. Dans cet exemple, je propose une régression LOESS (ou [GAM](https://en.wikipedia.org/wiki/Generalized_additive_model) s'il y a plus de 1500 points), ou une régression linéaire (modèle simple, logarithmique, ou polynomial de degré 2, 3 ou 4). Le paramètre se nomme __Type de modèle__.  

Notons deux points pour cette section. D'abord, je ne prends pas en compte les niveaux de la dimension qui détermine la couleur : on souhaite afficher une seule courbe de tendance afin de ne pas surcharger le graphique, un modèle simple fera l'affaire. Ensuite, je souhaite non seulement obtenir la courbe de tendance, mais aussi un intervalle de confiance (95%). Je souhaite également obtenir des indicateurs sur la qualité de la régression. Pour éviter de faire appel plusieurs fois à R, je concatène tous les résultats dans un seul champ de type chaîne de caractère que je pourrai ensuite parser dans Tableau.  

Le code est améliorable notamment au niveau de sa modularité... Je m'en occuperai peut être ultérieurement pour une mise à jour de l'article.  

Le format final est donc : _lower___<___fit___>___upper___|1|___Residual standard error___|2|___Multiple R-squared___|3|___Adjusted R-squared___|4|___F-test p-value_  
```{r Tableau_model, results="hide"}
# Tableau variables
# values are given as an example here, replace with args in Tableau
x <- insertion$femmes
y <- insertion$salaire_net_median_des_emplois_a_temps_plein
modelType <- "loess/gam" # as an example

# fixed arguments
confLvl <- 0.95 # confidence interval
loessLimit <- 1500 # n limit for computing LOESS

###########################
# remove missing values (only in R, do this with filters in Tableau)
complete <- complete.cases(x, y)
x <- x[complete]
y <- y[complete]
##########################

###### choose model ######
# LOESS/GAM case
if(modelType == "loess/gam") {
      # LOESS if dataset size is less than the limit
      if(length(y) <= loessLimit) {
            model <- loess(y ~ x)
            pred <- predict(model, se=TRUE)
            # prepare data for trend line
            trendLine <- paste0(
                  pred$fit - (qt(1 - (1 - confLvl) / 2, pred$df) * pred$se), 
                  "<",
                  pred$fit, 
                  ">",
                  pred$fit + (qt(1 - (1 - confLvl) / 2, pred$df) * pred$se)
            )
            # prepare data for summary
            SSE <- sum((y - pred$fit)^2) # sum of squares for error
            SST <- sum((y - mean(y))^2) # sum of squares total
            SSM <- sum((pred$fit - mean(y))^2) # sum of squares for model
            DFE <- pred$df # effective degrees of freedom
            r.squared <- 1 - (SSE / SST)
            adj.r.squared <- r.squared - (1 - r.squared) * (1 / (DFE - 1))
            fStat <- (SSM/1) / (SSE/DFE) # F-test
            pValue <- pf(fStat, 1, DFE, lower.tail=FALSE)
      } else {
            # GAM if dataset too big
            if (!require("mgcv")) {
                  install.packages("mgcv")
                  require(mgcv)
            }
            model <- gam(y ~ ti(x))
            pred <- predict(model, se.fit=TRUE)
            DFE <- length(y) - 1 # degrees of freedom
            # prepare data for trend line
            trendLine <- paste0(
                  pred$fit - (qt(1 - (1 - confLvl) / 2, DFE) * pred$se.fit), 
                  "<",
                  pred$fit, 
                  ">",
                  pred$fit + (qt(1 - (1 - confLvl) / 2, DFE) * pred$se.fit)
            )
            # prepare data for summary
            SSE <- sum((y - pred$fit)^2) # sum of squares for error
            SST <- sum((y - mean(y))^2) # sum of squares total
            SSM <- sum((pred$fit - mean(y))^2) # sum of squares for model
            r.squared <- 1 - (SSE / SST)
            adj.r.squared <- r.squared - (1 - r.squared) * (1 / (DFE - 1))
            fStat <- (SSM/1) / (SSE/DFE) # F-test
            pValue <- pf(fStat, 1, DFE, lower.tail=FALSE)
      }
      # return results to Tableau
      modelSummary <- paste0(
      "|1|Residual standard error: ", formatC(pred$residual.scale, digits=1, format="f"), 
      " on ", formatC(pred$df, digits=1, format="f"), " degrees of freedom", 
      "|2|Multiple R-squared: ",  formatC(r.squared, digits=4, format="f"),
      "|3|Adjusted R-squared: ",  formatC(adj.r.squared, digits=4, format="f"),
      "|4|F-test p-value: ",  formatC(pValue, digits=4, format="e")
      )
      paste0(trendLine, modelSummary)
# linear model cases
} else if(substr(modelType, 1, 2) == "lm") {
      # set formula
      lm_formula <- if(modelType == "lm") {
            # simple model case
            y ~ x
      } else if(substr(modelType, 4, 6) == "log") {
            # log case
            y ~ log(x)
      } else if(substr(modelType, 4, 6) == "pol") {
            # polynomial case (with degree 2 to 4)
            degree <- as.numeric(substr(modelType, 8, 8))
            if(degree == 2) {
                 y ~ x + I(x^2) 
            } else if(degree == 3) {
                 y ~ x + I(x^2) + I(x^3) 
            } else if(degree == 4) {
                 y ~ x + I(x^2) + I(x^3) + I(x^4)
            }
      }
      # compute linear model with chosen formula
      model <- lm(lm_formula)
      pred <- predict(model, interval="confidence", level=confLvl)
      # prepare data for trend line
      trendLine <- paste0(
            pred[, "lwr"],  
            "<",
            pred[, "fit"],  
            ">",
            pred[, "upr"]
      )
      # prepare data for summary
      mSum <- summary(model)
      # return results to Tableau
      modelSummary <- paste0(
            "|1|Residual standard error: ", formatC(mSum$sigma, digits=1, format="f"), 
            " on ", mSum$df[2], " degrees of freedom", 
            "|2|Multiple R-squared: ",  formatC(mSum$r.squared, digits=4, format="f"),
            "|3|Adjusted R-squared: ",  formatC(mSum$adj.r.squared, digits=4, format="f"),
            "|4|F-test p-value: ",  formatC(
                  pf(mSum$fstatistic[1], mSum$fstatistic[2], mSum$fstatistic[3], lower.tail=FALSE), 
                  digits=4, format="e")
      )
      paste0(trendLine, modelSummary)
}
```
<br>  

Quelques modifications minimes permettent d'interfacer ce code dans Tableau (champ __Script R : courbe de tendance__) en utilisant la fonction _SCRIPT_STR_. Il faut ensuite parser ce champ pour obtenir les différents éléments :  

* le champ __lwr__ pour la limite basse de la courbe de tendance  
* le champ __fit__ pour la courbe de tendance  
* le champ __upr__ pour la limite haute de la courbe de tendance  
* le champ __summary__ pour les détails du modèle  

Ensuite pour ajouter le tout dans la vue :  

1. glissez __Noms de mesures__ dans les filtres  
2. sélectionnez les champs __lwr__, __fit__ et __upr__  
3. glissez __Valeurs de mesures__ en lignes et créez un axe double (en synchronisant les axes)  
4. glissez __Noms de mesures__ en couleur dans le repère __Valeurs de mesures__, attribuez une couleur au champ __fit__ et une autre aux champs __lrw__ et __upr__  
5. enfin vous pouvez placer __summary__ en infobulle  
<br>  

#### Densités marginales
Sur notre nuage de points, les densité marginales sont les fonctions de densité de nos composantes X et Y. Par exemple si l'on prend une ligne au hasard dans notre jeu de données (soit une discipline pour une année et un établissement précis, valeurs nulles exclues) et que X est le pourcentage de femmes, l'aire sous la fonction de densité de X entre deux valeurs est la probabilité que le pourcentage de femmes soit compris entre ces deux valeurs. Ainsi si l'aire est de 0.25 entre 0% et 50%, cela signifie qu'il y a 25% de chances qu'il y ait moins d'une femme sur deux dans la discipline observée.  

Ainsi, afficher les densités marginales aux marges de notre nuage de points nous permet de visualiser la répartition des points sur les axes X et Y, même lorsque le graphique est très chargé et que de nombreux points se retrouvent superposés.  

...
```{r Tableau_density, results="hide"}
# Tableau variables
# values are given as an example here, replace with args in Tableau
vec <- x
axisMin <- min(vec)
axisMax <- max(vec)

###### calculated variables ######
# number of elements in the data
size <- length(vec)
# find next power of two for number of kernel density estimates
n_dens <- 2^ceiling(log2(size))
# choose smoothing bandwidth for the Gaussian kernel density estimator
bw_dens <- bw.nrd0(vec) 
# select the left and right-most points of the axis at which the density is to be estimated
from_dens <- min(vec) - 3 * bw_dens
if(from_dens < axisMin) {from_dens <- axisMin}
to_dens <- max(vec) + 3 * bw_dens
if(to_dens > axisMax) {to_dens <- axisMax}

# computes kernel density estimates
dens <- density(vec, bw = bw_dens, kernel = "gaussian", n = n_dens, from = from_dens, to = to_dens)

# perform cubic spline interpolation in order to have a number of points equal to the sample size
# (we use sample size minus two because we need to have a zero values on the left and right ends
# in order to draw polygons in Tableau)
coords <- spline(dens$x, dens$y, n = size - 2)
coords <- cbind(c(coords$x[1], coords$x, coords$x[size - 2]), 
                c(coords$y[1], coords$y, coords$y[size - 2]), 
                c(0, coords$y, 0))

# return as a string that will be parsed in tableau: x, y for lines, y for polygons
paste0(coords[,1], "l", coords[,2], "p", coords[,3])
```
<br>  

#### Alignement des axes et agencement des vues
...  
